Скрипт DAG'а Airflow - [airflow_script.py](airflow_script.py)  
Скрипт очистки данных - [data_cleaning.py](data_cleaning.py)  
<br>  
Airflow каждые сутки (здесь - 15 минут для более быстрого выполнения 3-х запусков) автоматически создаёт кластер из 4-х нод на сервисе Yandex Cloud. На кластере с помощью PySpark происходит распределённое выполнение скрипта очистки данных, результаты обработки сохраняются в S3-хранилище, после чего кластер удаляется для экономии финансов. Скрипт проводит оценку того, какие данные уже были обработаны, а какие поступили на обработку впервые. Обрабатываются только новые данные и добавляются к уже обработанным.  
<br>  
Скриншот интерфейса Airflow с успешно выполненным 3 раза DAG'ом:

<img src="https://github.com/mikhail-rozov/Otus-MLOps-course/assets/77928025/254ff711-dbf0-4f7d-9585-59df3b87b33b" width="750" height="421" />
